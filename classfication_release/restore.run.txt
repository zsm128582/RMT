| distributed init (rank 3): env://
| distributed init (rank 1): env://
| distributed init (rank 0): env://
| distributed init (rank 2): env://
Namespace(early_conv=False, conv_pos=False, use_ortho=False, batch_size=64, epochs=300, model='Restore', input_size=224, drop=0.0, drop_path=0.05, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', data_path='/home/zengshimao/code/RMT/data', data_set='IMNET', inat_category='name', output_dir='save', device='cuda', seed=0, resume='', start_epoch=0, eval=False, dist_eval=True, num_workers=32, pin_mem=True, world_size=4, dist_url='env://', rank=0, gpu=0, distributed=True, dist_backend='nccl')
Creating model: Restore
Restormer(
  (patch_embed): PatchEmbed(
    (proj): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): GELU(approximate='none')
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): GELU(approximate='none')
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): GELU(approximate='none')
      (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (encoder_level1): Sequential(
    (0): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (project_out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(64, 340, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(340, 340, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=340, bias=False)
        (project_out): Conv2d(170, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (1): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (project_out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(64, 340, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(340, 340, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=340, bias=False)
        (project_out): Conv2d(170, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (2): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (project_out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(64, 340, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(340, 340, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=340, bias=False)
        (project_out): Conv2d(170, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (3): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (project_out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(64, 340, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(340, 340, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=340, bias=False)
        (project_out): Conv2d(170, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
  (down1_2): Downsample(
    (body): Sequential(
      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): PixelUnshuffle(downscale_factor=2)
    )
  )
  (encoder_level2): Sequential(
    (0): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(128, 680, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(680, 680, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=680, bias=False)
        (project_out): Conv2d(340, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (1): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(128, 680, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(680, 680, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=680, bias=False)
        (project_out): Conv2d(340, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (2): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(128, 680, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(680, 680, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=680, bias=False)
        (project_out): Conv2d(340, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (3): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(128, 680, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(680, 680, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=680, bias=False)
        (project_out): Conv2d(340, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (4): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(128, 680, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(680, 680, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=680, bias=False)
        (project_out): Conv2d(340, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (5): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(128, 680, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(680, 680, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=680, bias=False)
        (project_out): Conv2d(340, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
  (down2_3): Downsample(
    (body): Sequential(
      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): PixelUnshuffle(downscale_factor=2)
    )
  )
  (encoder_level3): Sequential(
    (0): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (project_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(256, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360, bias=False)
        (project_out): Conv2d(680, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (1): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (project_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(256, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360, bias=False)
        (project_out): Conv2d(680, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (2): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (project_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(256, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360, bias=False)
        (project_out): Conv2d(680, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (3): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (project_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(256, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360, bias=False)
        (project_out): Conv2d(680, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (4): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (project_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(256, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360, bias=False)
        (project_out): Conv2d(680, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (5): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (project_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(256, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360, bias=False)
        (project_out): Conv2d(680, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
  (down3_4): Downsample(
    (body): Sequential(
      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): PixelUnshuffle(downscale_factor=2)
    )
  )
  (latent): Sequential(
    (0): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(512, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
        (project_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(512, 2722, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(2722, 2722, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2722, bias=False)
        (project_out): Conv2d(1361, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (1): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(512, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
        (project_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(512, 2722, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(2722, 2722, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2722, bias=False)
        (project_out): Conv2d(1361, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (2): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(512, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
        (project_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(512, 2722, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(2722, 2722, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2722, bias=False)
        (project_out): Conv2d(1361, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (3): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(512, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
        (project_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(512, 2722, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(2722, 2722, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2722, bias=False)
        (project_out): Conv2d(1361, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (4): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(512, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
        (project_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(512, 2722, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(2722, 2722, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2722, bias=False)
        (project_out): Conv2d(1361, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (5): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(512, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
        (project_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(512, 2722, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(2722, 2722, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2722, bias=False)
        (project_out): Conv2d(1361, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (6): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(512, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
        (project_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(512, 2722, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(2722, 2722, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2722, bias=False)
        (project_out): Conv2d(1361, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (7): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(512, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
        (project_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(512, 2722, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(2722, 2722, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2722, bias=False)
        (project_out): Conv2d(1361, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
  (up4_3): Upsample(
    (body): Sequential(
      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): PixelShuffle(upscale_factor=2)
    )
  )
  (reduce_chan_level3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (decoder_level3): Sequential(
    (0): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (project_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(256, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360, bias=False)
        (project_out): Conv2d(680, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (1): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (project_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(256, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360, bias=False)
        (project_out): Conv2d(680, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (2): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (project_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(256, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360, bias=False)
        (project_out): Conv2d(680, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (3): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (project_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(256, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360, bias=False)
        (project_out): Conv2d(680, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (4): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (project_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(256, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360, bias=False)
        (project_out): Conv2d(680, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (5): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (project_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(256, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360, bias=False)
        (project_out): Conv2d(680, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
  (up3_2): Upsample(
    (body): Sequential(
      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): PixelShuffle(upscale_factor=2)
    )
  )
  (reduce_chan_level2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (decoder_level2): Sequential(
    (0): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(128, 680, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(680, 680, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=680, bias=False)
        (project_out): Conv2d(340, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (1): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(128, 680, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(680, 680, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=680, bias=False)
        (project_out): Conv2d(340, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (2): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(128, 680, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(680, 680, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=680, bias=False)
        (project_out): Conv2d(340, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (3): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(128, 680, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(680, 680, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=680, bias=False)
        (project_out): Conv2d(340, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (4): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(128, 680, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(680, 680, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=680, bias=False)
        (project_out): Conv2d(340, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (5): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(128, 680, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(680, 680, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=680, bias=False)
        (project_out): Conv2d(340, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
  (up2_1): Upsample(
    (body): Sequential(
      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): PixelShuffle(upscale_factor=2)
    )
  )
  (decoder_level1): Sequential(
    (0): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(128, 680, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(680, 680, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=680, bias=False)
        (project_out): Conv2d(340, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (1): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(128, 680, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(680, 680, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=680, bias=False)
        (project_out): Conv2d(340, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (2): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(128, 680, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(680, 680, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=680, bias=False)
        (project_out): Conv2d(340, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (3): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(128, 680, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(680, 680, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=680, bias=False)
        (project_out): Conv2d(340, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
  (patchMerge): PatchMerge(
    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (proj): Linear(in_features=512, out_features=1024, bias=True)
  (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (swish): MemoryEfficientSwish()
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=1000, bias=True)
)
| module                         | #parameters or shape   | #flops     |
|:-------------------------------|:-----------------------|:-----------|
| model                          | 48.593M                | 10.575G    |
|  patch_embed.proj              |  65.952K               |  0.302G    |
|   patch_embed.proj.0           |   0.896K               |   10.838M  |
|    patch_embed.proj.0.weight   |    (32, 3, 3, 3)       |            |
|    patch_embed.proj.0.bias     |    (32,)               |            |
|   patch_embed.proj.1           |   64                   |   0.803M   |
|    patch_embed.proj.1.weight   |    (32,)               |            |
|    patch_embed.proj.1.bias     |    (32,)               |            |
|   patch_embed.proj.3           |   9.248K               |   0.116G   |
|    patch_embed.proj.3.weight   |    (32, 32, 3, 3)      |            |
|    patch_embed.proj.3.bias     |    (32,)               |            |
|   patch_embed.proj.4           |   64                   |   0.803M   |
|    patch_embed.proj.4.weight   |    (32,)               |            |
|    patch_embed.proj.4.bias     |    (32,)               |            |
|   patch_embed.proj.6           |   18.496K              |   57.803M  |
|    patch_embed.proj.6.weight   |    (64, 32, 3, 3)      |            |
|    patch_embed.proj.6.bias     |    (64,)               |            |
|   patch_embed.proj.7           |   0.128K               |   0.401M   |
|    patch_embed.proj.7.weight   |    (64,)               |            |
|    patch_embed.proj.7.bias     |    (64,)               |            |
|   patch_embed.proj.9           |   36.928K              |   0.116G   |
|    patch_embed.proj.9.weight   |    (64, 64, 3, 3)      |            |
|    patch_embed.proj.9.bias     |    (64,)               |            |
|   patch_embed.proj.10          |   0.128K               |   0.401M   |
|    patch_embed.proj.10.weight  |    (64,)               |            |
|    patch_embed.proj.10.bias    |    (64,)               |            |
|  encoder_level1                |  0.216M                |  0.778G    |
|   encoder_level1.0             |   54.069K              |   0.194G   |
|    encoder_level1.0.norm1.body |    0.128K              |    0       |
|    encoder_level1.0.attn       |    18.113K             |    82.489M |
|    encoder_level1.0.norm2.body |    0.128K              |    0       |
|    encoder_level1.0.ffn        |    35.7K               |    0.112G  |
|   encoder_level1.1             |   54.069K              |   0.194G   |
|    encoder_level1.1.norm1.body |    0.128K              |    0       |
|    encoder_level1.1.attn       |    18.113K             |    82.489M |
|    encoder_level1.1.norm2.body |    0.128K              |    0       |
|    encoder_level1.1.ffn        |    35.7K               |    0.112G  |
|   encoder_level1.2             |   54.069K              |   0.194G   |
|    encoder_level1.2.norm1.body |    0.128K              |    0       |
|    encoder_level1.2.attn       |    18.113K             |    82.489M |
|    encoder_level1.2.norm2.body |    0.128K              |    0       |
|    encoder_level1.2.ffn        |    35.7K               |    0.112G  |
|   encoder_level1.3             |   54.069K              |   0.194G   |
|    encoder_level1.3.norm1.body |    0.128K              |    0       |
|    encoder_level1.3.attn       |    18.113K             |    82.489M |
|    encoder_level1.3.norm2.body |    0.128K              |    0       |
|    encoder_level1.3.ffn        |    35.7K               |    0.112G  |
|  down1_2.body.0                |  18.432K               |  57.803M   |
|   down1_2.body.0.weight        |   (32, 64, 3, 3)       |            |
|  encoder_level2                |  1.237M                |  1.045G    |
|   encoder_level2.0             |   0.206M               |   0.174G   |
|    encoder_level2.0.norm1.body |    0.256K              |    0       |
|    encoder_level2.0.attn       |    68.994K             |    66.935M |
|    encoder_level2.0.norm2.body |    0.256K              |    0       |
|    encoder_level2.0.ffn        |    0.137M              |    0.107G  |
|   encoder_level2.1             |   0.206M               |   0.174G   |
|    encoder_level2.1.norm1.body |    0.256K              |    0       |
|    encoder_level2.1.attn       |    68.994K             |    66.935M |
|    encoder_level2.1.norm2.body |    0.256K              |    0       |
|    encoder_level2.1.ffn        |    0.137M              |    0.107G  |
|   encoder_level2.2             |   0.206M               |   0.174G   |
|    encoder_level2.2.norm1.body |    0.256K              |    0       |
|    encoder_level2.2.attn       |    68.994K             |    66.935M |
|    encoder_level2.2.norm2.body |    0.256K              |    0       |
|    encoder_level2.2.ffn        |    0.137M              |    0.107G  |
|   encoder_level2.3             |   0.206M               |   0.174G   |
|    encoder_level2.3.norm1.body |    0.256K              |    0       |
|    encoder_level2.3.attn       |    68.994K             |    66.935M |
|    encoder_level2.3.norm2.body |    0.256K              |    0       |
|    encoder_level2.3.ffn        |    0.137M              |    0.107G  |
|   encoder_level2.4             |   0.206M               |   0.174G   |
|    encoder_level2.4.norm1.body |    0.256K              |    0       |
|    encoder_level2.4.attn       |    68.994K             |    66.935M |
|    encoder_level2.4.norm2.body |    0.256K              |    0       |
|    encoder_level2.4.ffn        |    0.137M              |    0.107G  |
|   encoder_level2.5             |   0.206M               |   0.174G   |
|    encoder_level2.5.norm1.body |    0.256K              |    0       |
|    encoder_level2.5.attn       |    68.994K             |    66.935M |
|    encoder_level2.5.norm2.body |    0.256K              |    0       |
|    encoder_level2.5.ffn        |    0.137M              |    0.107G  |
|  down2_3.body.0                |  73.728K               |  57.803M   |
|   down2_3.body.0.weight        |   (64, 128, 3, 3)      |            |
|  encoder_level3                |  4.827M                |  0.983G    |
|   encoder_level3.0             |   0.805M               |   0.164G   |
|    encoder_level3.0.norm1.body |    0.512K              |    0       |
|    encoder_level3.0.attn       |    0.269M              |    59.158M |
|    encoder_level3.0.norm2.body |    0.512K              |    0       |
|    encoder_level3.0.ffn        |    0.534M              |    0.105G  |
|   encoder_level3.1             |   0.805M               |   0.164G   |
|    encoder_level3.1.norm1.body |    0.512K              |    0       |
|    encoder_level3.1.attn       |    0.269M              |    59.158M |
|    encoder_level3.1.norm2.body |    0.512K              |    0       |
|    encoder_level3.1.ffn        |    0.534M              |    0.105G  |
|   encoder_level3.2             |   0.805M               |   0.164G   |
|    encoder_level3.2.norm1.body |    0.512K              |    0       |
|    encoder_level3.2.attn       |    0.269M              |    59.158M |
|    encoder_level3.2.norm2.body |    0.512K              |    0       |
|    encoder_level3.2.ffn        |    0.534M              |    0.105G  |
|   encoder_level3.3             |   0.805M               |   0.164G   |
|    encoder_level3.3.norm1.body |    0.512K              |    0       |
|    encoder_level3.3.attn       |    0.269M              |    59.158M |
|    encoder_level3.3.norm2.body |    0.512K              |    0       |
|    encoder_level3.3.ffn        |    0.534M              |    0.105G  |
|   encoder_level3.4             |   0.805M               |   0.164G   |
|    encoder_level3.4.norm1.body |    0.512K              |    0       |
|    encoder_level3.4.attn       |    0.269M              |    59.158M |
|    encoder_level3.4.norm2.body |    0.512K              |    0       |
|    encoder_level3.4.ffn        |    0.534M              |    0.105G  |
|   encoder_level3.5             |   0.805M               |   0.164G   |
|    encoder_level3.5.norm1.body |    0.512K              |    0       |
|    encoder_level3.5.attn       |    0.269M              |    59.158M |
|    encoder_level3.5.norm2.body |    0.512K              |    0       |
|    encoder_level3.5.ffn        |    0.534M              |    0.105G  |
|  down3_4.body.0                |  0.295M                |  57.803M   |
|   down3_4.body.0.weight        |   (128, 256, 3, 3)     |            |
|  latent                        |  25.436M               |  1.271G    |
|   latent.0                     |   3.179M               |   0.159G   |
|    latent.0.norm1.body         |    1.024K              |    0       |
|    latent.0.attn               |    1.062M              |    55.269M |
|    latent.0.norm2.body         |    1.024K              |    0       |
|    latent.0.ffn                |    2.115M              |    0.104G  |
|   latent.1                     |   3.179M               |   0.159G   |
|    latent.1.norm1.body         |    1.024K              |    0       |
|    latent.1.attn               |    1.062M              |    55.269M |
|    latent.1.norm2.body         |    1.024K              |    0       |
|    latent.1.ffn                |    2.115M              |    0.104G  |
|   latent.2                     |   3.179M               |   0.159G   |
|    latent.2.norm1.body         |    1.024K              |    0       |
|    latent.2.attn               |    1.062M              |    55.269M |
|    latent.2.norm2.body         |    1.024K              |    0       |
|    latent.2.ffn                |    2.115M              |    0.104G  |
|   latent.3                     |   3.179M               |   0.159G   |
|    latent.3.norm1.body         |    1.024K              |    0       |
|    latent.3.attn               |    1.062M              |    55.269M |
|    latent.3.norm2.body         |    1.024K              |    0       |
|    latent.3.ffn                |    2.115M              |    0.104G  |
|   latent.4                     |   3.179M               |   0.159G   |
|    latent.4.norm1.body         |    1.024K              |    0       |
|    latent.4.attn               |    1.062M              |    55.269M |
|    latent.4.norm2.body         |    1.024K              |    0       |
|    latent.4.ffn                |    2.115M              |    0.104G  |
|   latent.5                     |   3.179M               |   0.159G   |
|    latent.5.norm1.body         |    1.024K              |    0       |
|    latent.5.attn               |    1.062M              |    55.269M |
|    latent.5.norm2.body         |    1.024K              |    0       |
|    latent.5.ffn                |    2.115M              |    0.104G  |
|   latent.6                     |   3.179M               |   0.159G   |
|    latent.6.norm1.body         |    1.024K              |    0       |
|    latent.6.attn               |    1.062M              |    55.269M |
|    latent.6.norm2.body         |    1.024K              |    0       |
|    latent.6.ffn                |    2.115M              |    0.104G  |
|   latent.7                     |   3.179M               |   0.159G   |
|    latent.7.norm1.body         |    1.024K              |    0       |
|    latent.7.attn               |    1.062M              |    55.269M |
|    latent.7.norm2.body         |    1.024K              |    0       |
|    latent.7.ffn                |    2.115M              |    0.104G  |
|  up4_3.body.0                  |  4.719M                |  0.231G    |
|   up4_3.body.0.weight          |   (1024, 512, 3, 3)    |            |
|  reduce_chan_level3            |  0.131M                |  25.69M    |
|   reduce_chan_level3.weight    |   (256, 512, 1, 1)     |            |
|  decoder_level3                |  4.827M                |  0.983G    |
|   decoder_level3.0             |   0.805M               |   0.164G   |
|    decoder_level3.0.norm1.body |    0.512K              |    0       |
|    decoder_level3.0.attn       |    0.269M              |    59.158M |
|    decoder_level3.0.norm2.body |    0.512K              |    0       |
|    decoder_level3.0.ffn        |    0.534M              |    0.105G  |
|   decoder_level3.1             |   0.805M               |   0.164G   |
|    decoder_level3.1.norm1.body |    0.512K              |    0       |
|    decoder_level3.1.attn       |    0.269M              |    59.158M |
|    decoder_level3.1.norm2.body |    0.512K              |    0       |
|    decoder_level3.1.ffn        |    0.534M              |    0.105G  |
|   decoder_level3.2             |   0.805M               |   0.164G   |
|    decoder_level3.2.norm1.body |    0.512K              |    0       |
|    decoder_level3.2.attn       |    0.269M              |    59.158M |
|    decoder_level3.2.norm2.body |    0.512K              |    0       |
|    decoder_level3.2.ffn        |    0.534M              |    0.105G  |
|   decoder_level3.3             |   0.805M               |   0.164G   |
|    decoder_level3.3.norm1.body |    0.512K              |    0       |
|    decoder_level3.3.attn       |    0.269M              |    59.158M |
|    decoder_level3.3.norm2.body |    0.512K              |    0       |
|    decoder_level3.3.ffn        |    0.534M              |    0.105G  |
|   decoder_level3.4             |   0.805M               |   0.164G   |
|    decoder_level3.4.norm1.body |    0.512K              |    0       |
|    decoder_level3.4.attn       |    0.269M              |    59.158M |
|    decoder_level3.4.norm2.body |    0.512K              |    0       |
|    decoder_level3.4.ffn        |    0.534M              |    0.105G  |
|   decoder_level3.5             |   0.805M               |   0.164G   |
|    decoder_level3.5.norm1.body |    0.512K              |    0       |
|    decoder_level3.5.attn       |    0.269M              |    59.158M |
|    decoder_level3.5.norm2.body |    0.512K              |    0       |
|    decoder_level3.5.ffn        |    0.534M              |    0.105G  |
|  up3_2.body.0                  |  1.18M                 |  0.231G    |
|   up3_2.body.0.weight          |   (512, 256, 3, 3)     |            |
|  reduce_chan_level2            |  32.768K               |  25.69M    |
|   reduce_chan_level2.weight    |   (128, 256, 1, 1)     |            |
|  decoder_level2                |  1.237M                |  1.045G    |
|   decoder_level2.0             |   0.206M               |   0.174G   |
|    decoder_level2.0.norm1.body |    0.256K              |    0       |
|    decoder_level2.0.attn       |    68.994K             |    66.935M |
|    decoder_level2.0.norm2.body |    0.256K              |    0       |
|    decoder_level2.0.ffn        |    0.137M              |    0.107G  |
|   decoder_level2.1             |   0.206M               |   0.174G   |
|    decoder_level2.1.norm1.body |    0.256K              |    0       |
|    decoder_level2.1.attn       |    68.994K             |    66.935M |
|    decoder_level2.1.norm2.body |    0.256K              |    0       |
|    decoder_level2.1.ffn        |    0.137M              |    0.107G  |
|   decoder_level2.2             |   0.206M               |   0.174G   |
|    decoder_level2.2.norm1.body |    0.256K              |    0       |
|    decoder_level2.2.attn       |    68.994K             |    66.935M |
|    decoder_level2.2.norm2.body |    0.256K              |    0       |
|    decoder_level2.2.ffn        |    0.137M              |    0.107G  |
|   decoder_level2.3             |   0.206M               |   0.174G   |
|    decoder_level2.3.norm1.body |    0.256K              |    0       |
|    decoder_level2.3.attn       |    68.994K             |    66.935M |
|    decoder_level2.3.norm2.body |    0.256K              |    0       |
|    decoder_level2.3.ffn        |    0.137M              |    0.107G  |
|   decoder_level2.4             |   0.206M               |   0.174G   |
|    decoder_level2.4.norm1.body |    0.256K              |    0       |
|    decoder_level2.4.attn       |    68.994K             |    66.935M |
|    decoder_level2.4.norm2.body |    0.256K              |    0       |
|    decoder_level2.4.ffn        |    0.137M              |    0.107G  |
|   decoder_level2.5             |   0.206M               |   0.174G   |
|    decoder_level2.5.norm1.body |    0.256K              |    0       |
|    decoder_level2.5.attn       |    68.994K             |    66.935M |
|    decoder_level2.5.norm2.body |    0.256K              |    0       |
|    decoder_level2.5.ffn        |    0.137M              |    0.107G  |
|  up2_1.body.0                  |  0.295M                |  0.231G    |
|   up2_1.body.0.weight          |   (256, 128, 3, 3)     |            |
|  decoder_level1                |  0.825M                |  2.991G    |
|   decoder_level1.0             |   0.206M               |   0.748G   |
|    decoder_level1.0.norm1.body |    0.256K              |    0       |
|    decoder_level1.0.attn       |    68.993K             |    0.319G  |
|    decoder_level1.0.norm2.body |    0.256K              |    0       |
|    decoder_level1.0.ffn        |    0.137M              |    0.429G  |
|   decoder_level1.1             |   0.206M               |   0.748G   |
|    decoder_level1.1.norm1.body |    0.256K              |    0       |
|    decoder_level1.1.attn       |    68.993K             |    0.319G  |
|    decoder_level1.1.norm2.body |    0.256K              |    0       |
|    decoder_level1.1.ffn        |    0.137M              |    0.429G  |
|   decoder_level1.2             |   0.206M               |   0.748G   |
|    decoder_level1.2.norm1.body |    0.256K              |    0       |
|    decoder_level1.2.attn       |    68.993K             |    0.319G  |
|    decoder_level1.2.norm2.body |    0.256K              |    0       |
|    decoder_level1.2.ffn        |    0.137M              |    0.429G  |
|   decoder_level1.3             |   0.206M               |   0.748G   |
|    decoder_level1.3.norm1.body |    0.256K              |    0       |
|    decoder_level1.3.attn       |    68.993K             |    0.319G  |
|    decoder_level1.3.norm2.body |    0.256K              |    0       |
|    decoder_level1.3.ffn        |    0.137M              |    0.429G  |
|  patchMerge                    |  1.625M                |  0.232G    |
|   patchMerge.conv1             |   0.148M               |   0.116G   |
|    patchMerge.conv1.weight     |    (128, 128, 3, 3)    |            |
|    patchMerge.conv1.bias       |    (128,)              |            |
|   patchMerge.bn1               |   0.256K               |   0.201M   |
|    patchMerge.bn1.weight       |    (128,)              |            |
|    patchMerge.bn1.bias         |    (128,)              |            |
|   patchMerge.conv2             |   0.295M               |   57.803M  |
|    patchMerge.conv2.weight     |    (256, 128, 3, 3)    |            |
|    patchMerge.conv2.bias       |    (256,)              |            |
|   patchMerge.bn2               |   0.512K               |   0.1M     |
|    patchMerge.bn2.weight       |    (256,)              |            |
|    patchMerge.bn2.bias         |    (256,)              |            |
|   patchMerge.conv3             |   1.18M                |   57.803M  |
|    patchMerge.conv3.weight     |    (512, 256, 3, 3)    |            |
|    patchMerge.conv3.bias       |    (512,)              |            |
|   patchMerge.bn3               |   1.024K               |   50.176K  |
|    patchMerge.bn3.weight       |    (512,)              |            |
|    patchMerge.bn3.bias         |    (512,)              |            |
|  proj                          |  0.525M                |  25.69M    |
|   proj.weight                  |   (1024, 512)          |            |
|   proj.bias                    |   (1024,)              |            |
|  norm                          |  2.048K                |  0.1M      |
|   norm.weight                  |   (1024,)              |            |
|   norm.bias                    |   (1024,)              |            |
|  head                          |  1.025M                |  1.024M    |
|   head.weight                  |   (1000, 1024)         |            |
|   head.bias                    |   (1000,)              |            |
number of params: 48592696
Start training for 300 epochs
Epoch: [0]  [   0/5004]  eta: 218 days, 12:42:52  lr: 0.000001  loss: 6.9095 (6.9095)  time: 3773.1760  data: 3759.5508  max mem: 21781
Epoch: [0]  [  10/5004]  eta: 23 days, 21:54:42  lr: 0.000001  loss: 6.9233 (6.9215)  time: 413.7130  data: 381.3066  max mem: 21781
Epoch: [0]  [  20/5004]  eta: 12 days, 12:21:31  lr: 0.000001  loss: 6.9233 (6.9223)  time: 39.1413  data: 21.7412  max mem: 21781
Epoch: [0]  [  30/5004]  eta: 8 days, 11:17:15  lr: 0.000001  loss: 6.9152 (6.9203)  time: 0.5129  data: 0.0002  max mem: 21781
Epoch: [0]  [  40/5004]  eta: 11 days, 21:10:59  lr: 0.000001  loss: 6.9143 (6.9184)  time: 196.1829  data: 183.2339  max mem: 21781
Epoch: [0]  [  50/5004]  eta: 9 days, 12:56:48  lr: 0.000001  loss: 6.9131 (6.9173)  time: 196.1944  data: 183.2339  max mem: 21781
Epoch: [0]  [  60/5004]  eta: 7 days, 23:09:14  lr: 0.000001  loss: 6.9181 (6.9189)  time: 0.5457  data: 0.0002  max mem: 21781
Epoch: [0]  [  70/5004]  eta: 10 days, 7:28:39  lr: 0.000001  loss: 6.9249 (6.9195)  time: 216.7648  data: 195.7998  max mem: 21781
Epoch: [0]  [  80/5004]  eta: 9 days, 6:39:53  lr: 0.000001  loss: 6.9148 (6.9182)  time: 234.7839  data: 209.1441  max mem: 21781
Epoch: [0]  [  90/5004]  eta: 8 days, 5:52:34  lr: 0.000001  loss: 6.9205 (6.9186)  time: 18.5735  data: 13.3444  max mem: 21781
Epoch: [0]  [ 100/5004]  eta: 9 days, 18:50:19  lr: 0.000001  loss: 6.9219 (6.9190)  time: 211.2768  data: 189.3463  max mem: 21781
Epoch: [0]  [ 110/5004]  eta: 8 days, 21:18:39  lr: 0.000001  loss: 6.9233 (6.9194)  time: 211.2650  data: 189.3464  max mem: 21781
Epoch: [0]  [ 120/5004]  eta: 8 days, 3:20:31  lr: 0.000001  loss: 6.9231 (6.9195)  time: 0.5311  data: 0.0002  max mem: 21781
Epoch: [0]  [ 130/5004]  eta: 9 days, 3:53:17  lr: 0.000001  loss: 6.9177 (6.9197)  time: 192.9476  data: 154.5290  max mem: 21781
Epoch: [0]  [ 140/5004]  eta: 8 days, 14:24:04  lr: 0.000001  loss: 6.9223 (6.9201)  time: 205.8657  data: 157.4691  max mem: 21781
Epoch: [0]  [ 150/5004]  eta: 8 days, 0:23:03  lr: 0.000001  loss: 6.9254 (6.9199)  time: 13.4573  data: 2.9403  max mem: 21781
Epoch: [0]  [ 160/5004]  eta: 8 days, 8:07:40  lr: 0.000001  loss: 6.9145 (6.9198)  time: 120.3106  data: 107.6485  max mem: 21781
Epoch: [0]  [ 170/5004]  eta: 8 days, 5:47:06  lr: 0.000001  loss: 6.9145 (6.9199)  time: 182.1182  data: 160.3274  max mem: 21781
Epoch: [0]  [ 180/5004]  eta: 7 days, 18:30:36  lr: 0.000001  loss: 6.9155 (6.9198)  time: 62.3420  data: 52.6791  max mem: 21781
Epoch: [0]  [ 190/5004]  eta: 7 days, 9:41:39  lr: 0.000001  loss: 6.9150 (6.9196)  time: 9.6581  data: 0.0002  max mem: 21781
Epoch: [0]  [ 200/5004]  eta: 7 days, 23:19:46  lr: 0.000001  loss: 6.9128 (6.9194)  time: 181.3072  data: 161.9102  max mem: 21781
Epoch: [0]  [ 210/5004]  eta: 7 days, 16:00:13  lr: 0.000001  loss: 6.9104 (6.9193)  time: 188.7173  data: 178.4548  max mem: 21781
Epoch: [0]  [ 220/5004]  eta: 7 days, 7:20:37  lr: 0.000001  loss: 6.9197 (6.9193)  time: 17.0760  data: 16.5448  max mem: 21781
